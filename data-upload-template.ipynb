{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "cell_execution_strategy": "setup",
      "provenance": [],
      "name": "data-upload-template"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import load"
      ],
      "metadata": {
        "id": "nBg3iD0y-8v9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from colabutils import *"
      ],
      "metadata": {
        "id": "qIwm_CNl_k71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading a file from Google Colab to the local machine; or uploading a file from the machine to Colab's instant memory"
      ],
      "metadata": {
        "id": "TObQPOlfn2p2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ACTION = 'upload'\n",
        "# ACTION = 'download' to download the file to the local machine\n",
        "# ACTION = 'upload' to upload a file from local machine to Google Colab's\n",
        "# instant memory\n",
        "\n",
        "FILE_TO_DOWNLOAD_FROM_COLAB = None\n",
        "# FILE_TO_DOWNLOAD_FROM_COLAB = None. This parameter is obbligatory when\n",
        "# action = 'download'.\n",
        "# Declare as FILE_TO_DOWNLOAD_FROM_COLAB the file that you want to download, with\n",
        "# the correspondent extension.\n",
        "# It should not be declared in quotes.\n",
        "# e.g. to download a dictionary named dict, FILE_TO_DOWNLOAD_FROM_COLAB = 'dict.pkl'\n",
        "# To download a dataframe named df, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'df.csv'\n",
        "# To export a model named keras_model, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'keras_model.h5'\n",
        "\n",
        "# Dictionary storing the uploaded files returned as colab_files_dict.\n",
        "# Simply modify this object on the left of the equality:\n",
        "colab_files_dict = upload_to_or_download_file_from_colab (action = ACTION, file_to_download_from_colab = FILE_TO_DOWNLOAD_FROM_COLAB)"
      ],
      "metadata": {
        "id": "ce3HH_f1n2Ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing the dataset"
      ],
      "metadata": {
        "id": "73wDRim4oDq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## WARNING: Use this function to load dataframes stored on Excel (xls, xlsx, xlsm, xlsb, odf, ods and odt),\n",
        "## JSON, txt, or CSV (comma separated values) files. Tables in webpages or html files can also be read.\n",
        "\n",
        "FILE_DIRECTORY_PATH = \"\"\n",
        "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory\n",
        "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\"\n",
        "# or FILE_DIRECTORY_PATH = \"folder\"\n",
        "\n",
        "FILE_NAME_WITH_EXTENSION = \"\"\n",
        "# FILE_NAME_WITH_EXTENSION - (string, in quotes): input the name of the file with the\n",
        "# extension. e.g. FILE_NAME_WITH_EXTENSION = \"file.xlsx\", or,\n",
        "# FILE_NAME_WITH_EXTENSION = \"file.csv\", \"file.txt\", or \"file.json\"\n",
        "# Again, the extensions may be: xls, xlsx, xlsm, xlsb, odf, ods, odt, json, txt or csv.\n",
        "# Also, html files and webpages may be also read.\n",
        "\n",
        "# You may input the path for an HTML file containing a table to be read; or\n",
        "# a string containing the address for a webpage containing the table. The address must start\n",
        "# with www or htpp. If a website is input, the full address can be input as FILE_DIRECTORY_PATH\n",
        "# or as FILE_NAME_WITH_EXTENSION.\n",
        "\n",
        "LOAD_TXT_FILE_WITH_JSON_FORMAT = False\n",
        "# LOAD_TXT_FILE_WITH_JSON_FORMAT = False. Set LOAD_TXT_FILE_WITH_JSON_FORMAT = True\n",
        "# if you want to read a file with txt extension containing a text formatted as JSON\n",
        "# (but not saved as JSON).\n",
        "# WARNING: if LOAD_TXT_FILE_WITH_JSON_FORMAT = True, all the JSON file parameters of the\n",
        "# function (below) must be set. If not, an error message will be raised.\n",
        "\n",
        "HOW_MISSING_VALUES_ARE_REGISTERED = None\n",
        "# HOW_MISSING_VALUES_ARE_REGISTERED = None: keep it None if missing values are registered as None,\n",
        "# empty or np.nan. Pandas automatically converts None to NumPy np.nan objects (floats).\n",
        "# This parameter manipulates the argument na_values (default: None) from Pandas functions.\n",
        "# By default the following values are interpreted as NaN: ‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’,\n",
        "#‘-1.#QNAN’, ‘-NaN’, ‘-nan’, ‘1.#IND’, ‘1.#QNAN’, ‘<NA>’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’,\n",
        "# ‘n/a’, ‘nan’, ‘null’.\n",
        "\n",
        "# If a different denomination is used, indicate it as a string. e.g.\n",
        "# HOW_MISSING_VALUES_ARE_REGISTERED = '.' will convert all strings '.' to missing values;\n",
        "# HOW_MISSING_VALUES_ARE_REGISTERED = 0 will convert zeros to missing values.\n",
        "\n",
        "# If dict passed, specific per-column NA values. For example, if zero is the missing value\n",
        "# only in column 'numeric_col', you can specify the following dictionary:\n",
        "# how_missing_values_are_registered = {'numeric-col': 0}\n",
        "\n",
        "\n",
        "HAS_HEADER = True\n",
        "# HAS_HEADER = True if the the imported table has headers (row with columns names).\n",
        "# Alternatively, HAS_HEADER = False if the dataframe does not have header.\n",
        "\n",
        "DECIMAL_SEPARATOR = '.'\n",
        "# DECIMAL_SEPARATOR = '.' - String. Keep it '.' or None to use the period ('.') as\n",
        "# the decimal separator. Alternatively, specify here the separator.\n",
        "# e.g. DECIMAL_SEPARATOR = ',' will set the comma as the separator.\n",
        "# It manipulates the argument 'decimal' from Pandas functions.\n",
        "\n",
        "TXT_CSV_COL_SEP = \"comma\"\n",
        "# txt_csv_col_sep = \"comma\" - This parameter has effect only when the file is a 'txt'\n",
        "# or 'csv'. It informs how the different columns are separated.\n",
        "# Alternatively, txt_csv_col_sep = \"comma\", or txt_csv_col_sep = \",\"\n",
        "# for columns separated by comma;\n",
        "# txt_csv_col_sep = \"whitespace\", or txt_csv_col_sep = \" \"\n",
        "# for columns separated by simple spaces.\n",
        "# You can also set a specific separator as string. For example:\n",
        "# txt_csv_col_sep = '\\s+'; or txt_csv_col_sep = '\\t' (in this last example, the tabulation\n",
        "# is used as separator for the columns - '\\t' represents the tab character).\n",
        "\n",
        "## Parameters for loading Excel files:\n",
        "\n",
        "LOAD_ALL_SHEETS_AT_ONCE = False\n",
        "# LOAD_ALL_SHEETS_AT_ONCE = False - This parameter has effect only when for Excel files.\n",
        "# If LOAD_ALL_SHEETS_AT_ONCE = True, the function will return a list of dictionaries, each\n",
        "# dictionary containing 2 key-value pairs: the first key will be 'sheet', and its\n",
        "# value will be the name (or number) of the table (sheet). The second key will be 'df',\n",
        "# and its value will be the pandas dataframe object obtained from that sheet.\n",
        "# This argument has preference over SHEET_TO_LOAD. If it is True, all sheets will be loaded.\n",
        "\n",
        "SHEET_TO_LOAD = None\n",
        "# SHEET_TO_LOAD - This parameter has effect only when for Excel files.\n",
        "# keep SHEET_TO_LOAD = None not to specify a sheet of the file, so that the first sheet\n",
        "# will be loaded.\n",
        "# SHEET_TO_LOAD may be an integer or an string (inside quotes). SHEET_TO_LOAD = 0\n",
        "# loads the first sheet (sheet with index 0); SHEET_TO_LOAD = 1 loads the second sheet\n",
        "# of the file (index 1); SHEET_TO_LOAD = \"Sheet1\" loads a sheet named as \"Sheet1\".\n",
        "# Declare a number to load the sheet with that index, starting from 0; or declare a\n",
        "# name to load the sheet with that name.\n",
        "\n",
        "## Parameters for loading JSON files:\n",
        "\n",
        "JSON_RECORD_PATH = None\n",
        "# JSON_RECORD_PATH (string): manipulate parameter 'record_path' from json_normalize method.\n",
        "# Path in each object to list of records. If not passed, data will be assumed to\n",
        "# be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
        "# dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
        "# 'books' stores a nested JSON, declare, JSON_RECORD_PATH = 'books'\n",
        "\n",
        "JSON_FIELD_SEPARATOR = \"_\"\n",
        "# JSON_FIELD_SEPARATOR = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
        "# Nested records will generate names separated by sep.\n",
        "# e.g., for JSON_FIELD_SEPARATOR = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
        "# Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
        "# the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
        "# separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
        "\n",
        "JSON_METADATA_PREFIX_LIST = None\n",
        "# JSON_METADATA_PREFIX_LIST: list of strings (in quotes). Manipulates the parameter\n",
        "# 'meta' from json_normalize method. Fields to use as metadata for each record in resulting\n",
        "# table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
        "# will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
        "\n",
        "# e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
        "# 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
        "# Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
        "# are 'name' and 'last'.\n",
        "# Then, JSON_RECORD_PATH = 'books'\n",
        "# JSON_METADATA_PREFIX_LIST = ['name', 'last']\n",
        "\n",
        "\n",
        "# The dataframe will be stored in the object named 'dataset':\n",
        "# Simply modify this object on the left of equality:\n",
        "dataset = load_pandas_dataframe (file_directory_path = FILE_DIRECTORY_PATH, file_name_with_extension = FILE_NAME_WITH_EXTENSION, load_txt_file_with_json_format = LOAD_TXT_FILE_WITH_JSON_FORMAT, how_missing_values_are_registered = HOW_MISSING_VALUES_ARE_REGISTERED, has_header = HAS_HEADER, decimal_separator = DECIMAL_SEPARATOR, txt_csv_col_sep = TXT_CSV_COL_SEP, load_all_sheets_at_once = LOAD_ALL_SHEETS_AT_ONCE, sheet_to_load = SHEET_TO_LOAD, json_record_path = JSON_RECORD_PATH, json_field_separator = JSON_FIELD_SEPARATOR, json_metadata_prefix_list = JSON_METADATA_PREFIX_LIST)\n",
        "\n",
        "# OBS: If an Excel file is loaded and LOAD_ALL_SHEETS_AT_ONCE = True, then the object\n",
        "# dataset will be a list of dictionaries, with 'sheet' as key containing the sheet name; and 'df'\n",
        "# as key correspondent to the Pandas dataframe. So, to access the 3rd dataframe (index 2, since\n",
        "# indexing starts from zero): df = dataframe[2]['df'], where dataframe is the list returned."
      ],
      "metadata": {
        "id": "mfybjIEGoGXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Query a table from Google BigQuery using SQL and magic commands\n",
        "- You may load a BigQuery table or view as Pandas DataFrame.\n",
        "- It may be used to compare BigQuery table or join it with the uploaded data, for instance."
      ],
      "metadata": {
        "id": "_j4iJGdHC0rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ATTENTION: Replace MY_BIGQUERY_PROJECT with the project that will be billed.\n",
        "# -- This code will create a pd.DataFrame named results. You may replace\n",
        "#     results with any other object name.\n",
        "# Replace the SELECT line with any SQL query you want.\n",
        "# Replace MY_BIGQUERY_PROJECT.MY_DATASET.MY_TABLE_OR_VIEW with the correspondent\n",
        "# information from your data: MY_DATASET is the dataset name, and\n",
        "# MY_TABLE_OR_VIEW is the view or table name.\n",
        "%%bigquery results --project MY_BIGQUERY_PROJECT\n",
        "SELECT * FROM `MY_BIGQUERY_PROJECT.MY_DATASET.MY_TABLE_OR_VIEW`"
      ],
      "metadata": {
        "id": "pPK5ZQhdDHoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The queried data was saved in a Pandas DataFrame named results:\n",
        "results"
      ],
      "metadata": {
        "id": "eANvgFxhEKaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exporting the dataframe as CSV file (to notebook's workspace)"
      ],
      "metadata": {
        "id": "-gv1xTLvY1St"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## WARNING: all files exported from this function are .csv (comma separated values)\n",
        "\n",
        "DATAFRAME_OBJ_TO_BE_EXPORTED = dataset\n",
        "# Alternatively: object containing the dataset to be exported.\n",
        "# DATAFRAME_OBJ_TO_BE_EXPORTED: dataframe object that is going to be exported from the\n",
        "# function. Since it is an object (not a string), it should not be declared in quotes.\n",
        "# example: DATAFRAME_OBJ_TO_BE_EXPORTED = dataset will export the dataset object.\n",
        "# ATTENTION: The dataframe object must be a Pandas dataframe.\n",
        "\n",
        "FILE_DIRECTORY_PATH = \"\"\n",
        "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory\n",
        "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\"\n",
        "# or FILE_DIRECTORY_PATH = \"folder\"\n",
        "# If you want to export the file to AWS S3, this parameter will have no effect.\n",
        "# In this case, you can set FILE_DIRECTORY_PATH = None\n",
        "\n",
        "NEW_FILE_NAME_WITHOUT_EXTENSION = \"dataset\"\n",
        "# NEW_FILE_NAME_WITHOUT_EXTENSION - (string, in quotes): input the name of the\n",
        "# file without the extension. e.g. set NEW_FILE_NAME_WITHOUT_EXTENSION = \"my_file\"\n",
        "# to export the CSV file 'my_file.csv' to notebook's workspace.\n",
        "\n",
        "export_pd_dataframe_as_csv (dataframe_obj_to_be_exported = DATAFRAME_OBJ_TO_BE_EXPORTED, new_file_name_without_extension = NEW_FILE_NAME_WITHOUT_EXTENSION, file_directory_path = FILE_DIRECTORY_PATH)"
      ],
      "metadata": {
        "id": "QWCYL-WuY6V5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Copy the new file to Google Cloud Storage\n",
        "- It will allow further conversion to BigQuery table\n",
        "- Data can be stored and downloaded from GCS\n",
        "\n"
      ],
      "metadata": {
        "id": "0kqlbcdQCByb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"\"\n",
        "# file_path (str): file path in the root directory where the file is initially saved.\n",
        "# Examples: \"data.csv\" will export a file named \"data.csv\" in the root; \"folder/data.csv\" will export\n",
        "# a file in the \"folder\" directory.\n",
        "\n",
        "\"\"\"\n",
        "ATTENTION: If you try to copy a file that does not exist (i.e., if you put an invalid name), the code will\n",
        "run, but the file will not appear in GCS. If you are trying to copy a file, but cannot visualize it in\n",
        "GCS console, check if its name is correct.\n",
        "\"\"\"\n",
        "\n",
        "bucket = \"my-bucket\"\n",
        "# bucket (str): bucket name. Example: \"my-bucket\" will export to a bucket named my-bucket.\n",
        "bucket_folder = \"\"\n",
        "# bucket_folder (str): if the file should be exported to a particular folder or directory of the bucket,\n",
        "# declare here. If None or empty string, the file will be exported to the main directory of the bucket.\n",
        "# Examples: \"folder\" and bucket = \"my-bucket\" will export to \"my-bucket/folder\". If \"folder/subfolder\" will export to\n",
        "#\"my-bucket/folder/subfolder\".\n",
        "\n",
        "copy_to_gcs (file_path, bucket, bucket_folder = bucket_folder)"
      ],
      "metadata": {
        "id": "f6LlPw2O8rit"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}